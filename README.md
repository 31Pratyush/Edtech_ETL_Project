
# ğŸ“ EdTech Data Pipeline Project with PySpark & Redshift

This project implements a **real-world ETL pipeline** for an EdTech platform using **PySpark** and **Amazon Redshift**. It processes both static and dynamic datasets, models them into a **star schema**, and supports analytical queries for student engagement, quiz performance, and video activity tracking.

---

## ğŸ¯ Project Objective

The goal of this project is to simulate a **daily batch data pipeline** for an EdTech company that ingests, processes, and loads data into a star schema for analytical consumption.

### Key Objectives:

- ğŸ” **Daily ingestion** of time-partitioned CSV/JSON data with `batch_date` embedded in filenames.
- ğŸ“… Ensure **idempotent processing** using `date_key` columns for safe re-runs.
- ğŸ§¹ Apply data quality checks: null validation, value filters, deduplication.
- ğŸ” Use **surrogate keys (SHA2)** for dimension modeling and referential joins.
- ğŸ—ƒï¸ Load transformed data into **Redshift staging tables** with `overwrite` mode.
- ğŸ”„ Perform **MERGE for dimensions** and **delete-insert for fact tables** using Redshift SQL.
- â­ Create a **star schema** optimized for BI/reporting use cases.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ EdTech_pipeline.py  # Main PySpark ETL pipeline
â”œâ”€â”€ README.md  # Project documentation
â”œâ”€â”€ HLD
â””â”€â”€ sample data
```

---

## ğŸ—ï¸ Architecture Overview

**Tech Stack:**
- ğŸ§ª PySpark (Databricks/Spark cluster)
- â˜ï¸ AWS S3 (simulated with /FileStore/tables)
- ğŸ›¢ï¸ Amazon Redshift (DWH)
- ğŸ“Š Star Schema for BI/Analytics

**Pipeline Flow:**

```
S3 (CSV, JSON) 
    â†“
PySpark (Read â†’ Validate â†’ Transform)
    â†“
Redshift Staging Tables (overwrite mode)
    â†“
Redshift Merge Logic (upsert dim / delete-insert fact)
    â†“
Redshift Final Star Schema (fact + dimensions)
```

---

## ğŸ“š Datasets Processed

- `students.csv` â€“ student profiles (static)
- `instructors.csv` â€“ instructor master data (static)
- `courses_large.csv` â€“ course catalog (static)
- `student_engagement_<batch_date>.csv` â€“ daily engagement logins, discussions, assignments
- `quiz_attempts_<batch_date>.csv` â€“ daily quiz scores
- `video_watch_logs_<batch_date>.json` â€“ daily video watch activity

---

## ğŸ› ï¸ Features Implemented

âœ… Schema enforcement with PySpark  
âœ… Null check validations & data quality logic  
âœ… Quarantine of bad records  
âœ… Business logic: quiz score filters, event scoring, deduplication  
âœ… Surrogate key generation using `sha2()`  
âœ… `date_key` for idempotent batch processing  
âœ… Redshift `staging` table writes with `overwrite` mode  
âœ… Redshift `MERGE` & `DELETE-INSERT` logic for upserts  
âœ… Star Schema with `dim_student`, `dim_course`, `fact_quiz`, `fact_student_engagement`, etc.

---

## ğŸ—ƒï¸ Redshift Tables

### Dimension Tables
- `dim_student`
- `dim_instructor`
- `dim_course`

### Fact Tables
- `fact_student_engagement`
- `fact_quiz`
- `fact_video_interaction`

### Staging Tables
- `staging_dim_*`
- `staging_fact_*`

---

## ğŸ§ª Sample BI Queries

```sql
-- Top scoring students
SELECT student_name, AVG(quiz_score) 
FROM fact_quiz 
JOIN dim_student USING(student_id)
GROUP BY student_name;

-- Course login trends
SELECT course_name, date_key, COUNT(*) 
FROM fact_student_engagement 
JOIN dim_course USING(course_id)
WHERE event_type = 'login'
GROUP BY course_name, date_key;
```

---

## ğŸš€ Future Enhancements

- Integrate **Airflow orchestration**
- Automate Redshift MERGE logic via scripts
- Add **unit testing** for data quality
- Store bad records in S3/Delta Lake for audit

---

## ğŸ“‚ How to Run

1. Load datasets into `/FileStore/tables/` (Databricks) or replace with S3 paths.
2. Set `batch_date` in the script to match incoming files.
3. Run `EdTech.py` notebook/script.
4. Ensure Redshift credentials are configured for staging writes.
5. Use the generated MERGE SQL to populate final schema.

---

## ğŸ‘¨â€ğŸ’» Author

**Pratyush Sahay**
pratyush(dot)310899@gmail(dot)com  
Data Engineer | 3 Years Experience  
LinkedIn: [www.linkedin.com/in/pratyush-sahay]

---
